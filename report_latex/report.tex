\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath,calc}
\usepackage{hyperref}

\usepackage{pdfpages}
\usepackage{mdwlist}
\setcounter{MaxMatrixCols}{20}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstdefinestyle{myScalastyle}{
  frame=tb,
  language=scala,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  frame=single,
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\title{Music Recommendation using Recurrent Neural Networks}
\usepackage{tikz}
\usetikzlibrary{arrows}

\author{
Avishkar Bhoopchand\\
Department of Computer Science\\
MSc Computational Statistics and Machine Learning\\
\texttt{EMAIL@ucl.ac.uk} \\
\And
Fahad Syed\\
Department of Computer Science\\
MSc Computational Statistics and Machine Learning\\
\texttt{fahad.syed.15@ucl.ac.uk} \\
\And
Hipolito Iturraspe\\
Department of Computer Science\\
MSc Machine Learning\\
\texttt{hipolito.iturraspe.15@ucl.ac.uk} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle
\clearpage

\section{Introduction}
In this project we are looking at how to use a recurrent neural network for collabrative filtering, in particular, given a sequence of songs that a user has previously listened to we want to predict what songs the user will most likely listen to next.


%Library: Tensorflow (https://www.tensorflow.org/)
%Code based on Penn Tree Bank RNN example: https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html#recurrent-neural-networks
%Uses LSTM RNN cells: http://colah.github.io/posts/2015-08-Understanding-LSTMs/
%Using optimiser: Adam Optimiser (reference: http://arxiv.org/pdf/1412.6980v8.pdf) - this is a variation of Stochastic Gradient Descent
%Loss function is cross entropy loss - effectively the log of "perplexity" for each example in the sequence - intuitively, how many songs does the model think is likely to follow, a low perplexity means the model is very certain about its prediction, hence low perplexity is desirable. <-- Will want to mention why this is different from the evaluation metrics
%
%IDEA based on the following article:
%http://erikbern.com/2014/06/28/recurrent-neural-networks-for-collaborative-filtering/ 


%NB: Now using this dataset: http://www.cs.cornell.edu/~shuochen/lme/data_page.html
%The advantage of this dataset over the last.fm one is that this one has separated playlists and also they've already done some data preprocessing to convert songs to ids and have each playlist on a separate line which makes it easier to use :+1: 

\section{Recurrent Neural Networks}
Recurrent Neural Networks (RNNs) are a deep learning architecture ideally suited for sequential input data. By forming loops, the network is able to remember information about earlier data in the sequence in order to make better predictions. We feed the network a playlist as input and let it learn to predict the same playlist offset by 1 as a target. In this way, the network learns to predict the next song(s) a user may want to play based on the songs they've played so far in a particular playlist. \cite{LSTM}

\subsection{LSTM Cells}
Vanilla RNNs use a simple non-linearity such as sigmoid or tanh in each of the cells in sequence. These vanilla RNNs are effective at using recent information for each current prediction task, but tend to quickly "forget" information from earlier in the sequence. In order to overcome this issue, more complex cells capable of modelling memory are used. These Long Short Term Memory (LSTM) cells, figure ~\ref{LSTM_cell}, contain a cell memory vector. They receive as input the current input data as well as the state from the previous LSTM cell in the chain. Using these inputs, the cell can decide whether to read from, write to or erase its memory using 3 soft "gates" namely the input gate, output gate and forget gate. It also decides how much of the input vs how much of its memory it wants to pass along the chain as the state, or to the output. \cite{LSTM} 

\subsection{Architecture}
The architecture of the model we built for this task is illustrated in Figure X (insert architecture image from poster). A sequence of input songs from a playlist are passed as inputs to an RNN's input cells. The embedding vector for each song is retrieved and these are passed into an LSTM cell, along with a state vector representing the internal state of the LSTM up to the particular point in the sequence. The output of the LSTM is a vector which is mapped using softmax to a probability distribution over which song the network thinks is most likely to follow after a particular point in the sequence. Note that although it is illustrated (and implemented) as if the entire sequence is passed in one go, it is best to think of the network as processing each song in the sequence one at a time, making a prediction before seeing the next song. In other words, only information from the current song in the sequence and information the network has chosen to remember about earlier songs in the sequence are used for making a prediction at any point.  

Multiple layers of LSTM cells can be stacked in order to make the network deep. 
\begin{figure}
\label{LSTM_cell}
	\begin{center}
		\includegraphics[scale = 0.4]{"../images/LSTM_cell"}
		\caption{Digram showing the inner workings of an LSTM cell, our model uses multiple of these cells.}
	\end{center}
\end{figure}

\subsection{Parameters and Training}
The parameters of the network include embedding vectors for each song, parameters for the 3 gates of the LSTM cells (a matrix and bias vector) separate for each layer and a matrix and bias parameter for the output softmax layer. The parameters are trained by comparing the output predictions of the network with the actual targets. The cross entropy loss is computed (formula below) and the derivative of this is fed back into the network by backpropagation in order to update the network parameters. \cite{NN_deeplearning}

\begin{equation}
C = -\frac{1}{n} \sum y \log a + (1 - y) \log(1-a)
\end{equation}

\section{Implementation}
%The model described in the methods section was implemented in Python using Google's Tensorflow deep learning library [Reference: https://www.tensorflow.org]. The code for the model implementation was based to an extent on the Penn Tree Bank Language Modelling tutorial [Reference: https://www.tensorflow.org/versions/r0.8/tutorials/recurrent/index.html#recurrent-neural-networks]. 

\subsection{Batching}
In order to train efficiently using Stochastic Gradient Descent, batching is used whereby a batch of multiple playlists (the size of which is configurable) are passed to the network at each iteration. An issue arises due to different playlists having different lengths in the same batch. Luckily Tensorflow provides a principled way of dealing with this by allowing one to specify the lengths of each sequence in a batch. The RNNs zero their predictions when they reach the size limit of each sequence in a batch. 

\subsection{Training Method}
The model is trained using a variation of Stochastic Gradient Descent called the Adam Optimiser (an implementation of which comes with Tensorflow). This is an efficient optimiser that uses first-order gradients and is able to automatically adapt its learning rate using first and second order moments \cite{Adams}. We therefore did not need to implement a learning rate decay schedule that would usually be required for stochastic gradient descent. In order to address the possibility of exploding gradients during training, gradient clipping was employed, whereby the magnitude of gradients are clipped if they exceed a threshold. 

\subsection{Configuration}
The model we implemented used 2 layers, hidden state vector and embedding vector dimensions of 100, a batch size of 100, a gradient clipping threshold of 5 and an unravelled sequence length of 20 songs. 

\section{Results and Evaluation - might need different heading}
\subsection{Comparison to item based recommendation systems}
It should be noted that our approach has a subtle difference, from regular item based recommendation systems, on how the model is evaluated. Usually the model tried to predict a product to recommend and it is evaluated as successful if the prediction is in a set of targets. This allows the model to be evaluated using precision/recall/F-tests. In our approach, we are using only one target, do those metric don't hold for us. This a product of using an RNN and how we wanted to model the playlists. Furthermore, when recommending music the cost to the user for listening to a song or part of the song is much lower than say purchasing a product or watching a movie. 

The evaluation we proposed is looking at the the number of correct prediction, i.e. the song is one of the top K predictions is a successful result. We divide this by the number of total predictions to get overall accuracy of the model.

\begin{equation}
\text{accuracy@k} = \frac{\text{number of targets are in the top K predictions}}{\text{total number of predictions}}
\end{equation}
\subsection{Results}

We evaluate the model on the MIT test set. This test set has around around five times larger than our training set. - this might help the model not overfit?

Running the evaluation with varying values of K we have the results below 
[inset results table or just result] 
We can see that for k=5 we have a accuracy of around 10\% which in comparison to randomly selecting a song which would be around <0.1\% much better. Increasing K we see that the accuracy of the model increases reaching 70\% when we choose K to be 500. The total number of songs is around 9000, considering this we see that the results do give good results. 

 - Talk about comparing this to other recomendation system?
 - possible rephrasing the above paragraph


- section below may not be needed but might be worth putting in, still incomplete

\subsection{Possible steps to improve}
There are definitely ways we can improve the model, currently the training the model is slow which makes parameter selection difficult. Finding a better a simple but
Scalability, possibly hard to scale, unless there is some recursive RNN alg. 

Metric used in RNN different from actual metric used to measure performance.
	hard to find the gradient of day recall.
Using no a large dataset

The effect of many of these properties on the user experience is unclear, and depends on the application, without doing extensive online testing it is hard to tell which features to use to measure recommender systems, that have the most benefit to the user.

Users need to trust the system. Hard problem. How long we should recommend a song if a user does not click on it.

recommendations - youtube does this like crazy, doesn’t really help in all honesty.
Novelty - recommend items that the user did not know about
Serendipity - how successful the recommends are; possible hard to evaluate. 
Scalability 
Robustness
How to calculate the cost benefit and false/true positive/negative - hard task
Due to our offline learning we do not know if what we recommend is something that the user will actually wants, if they don’t click on it does not mean that the user does not like or want to the content as they may have just not looked at it. And if they clicked on it does not mean that they actually want that product.

More time available, we could train a model to predict latent representations of songs using the actual audio signal and this would all use to predict new song by running them through a deep neural network and recommending them to users based on similarity


\section{Code structure}
The files we implemented for this project are listed below. Please see the comments in the files themselves for further details. 
\begin{description}
\item [musicmodel.py] This file provides the MusicModel class which constructs a tensorflow computation graph which represents the recurrent neural network unravelled for a configurable number of steps. Various placeholder operators from the computation graph are made publically visible so that the relevant input data, targets and initial state can be passed and the final costs (loss) can be accessed.
\item [modeltrainer.py] This file provides the Trainer class which is used to train the model. The train function runs a configurable number of training epochs and after each epoch calls a list of hooks which display information about the training progress. Once training has completed, the model parameters are saved to a file so that the trained model can be later restored and evaluated. Each epoch consists of running through the batches in the training data, passing these to the model and then evaluating the model's cost function and training operator to invoke backpropagation. If the sequences in the batch are too long, they are broken up into sequence\textunderscore length chunks which are fed one at a time to the network, using the final state of the last chunk as the initial state.
\item [yes\textunderscore reader.py] This file loads the data set which consists of a separate training and test file. The train file is split into a training and validation set. Iterators are provided which split the training data into batches of the correct sequence length. 
\item [hooks.py] This file implements two hooks that output data during training. The first is a LossHook which merely reports the training set loss at the end of an epoch. The second runs the validation dataset through the network, calculates the loss for this dataset and reports it. The advantage of using such a hook is that the generalisation ability of the network can be assessed while it is being trained. 
\item [deepcf.py] This file pulls the various peices together. Configuration of the model can be specified here and the training or evaluation can be invoked using command line arguments.
\end{description}

\newpage

\begin{thebibliography}{9}

\bibitem{LSTM}
  Christopher Olah
  \emph{Undetstanding LSTM},
\url{http://colah.github.io/posts/2015-08-Understanding-LSTMs/}

\bibitem{NN_deeplearning}
  Michael Nielsen
  \emph{Neural Networks and Deep Learning},
\url{http://neuralnetworksanddeeplearning.com/chap3.html}

\bibitem{Adams}
  Diederik P. Kingma, Jimmy Lei Ba
  \emph{Adam: A Method For Stochastic Optimization},
http://arxiv.org/pdf/1412.6980v8.pdf
\end{thebibliography}

\end{document}

